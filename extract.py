# Script to extract various layers' outputs of the whisper model
# Aidan Seidle - 6/16/2025

# Imports
import torch
import numpy as np
import os
import subprocess

from tqdm import tqdm

import whisper

import pickle

os.environ["PATH"] += os.pathsep + r"C:/Users/ajseidle/scoop/apps/ffmpeg/current/bin"

# Can run to check ffmpeg is properly on the path
#subprocess.run(['ffmpeg', '-version'], check=True)

# Custom encoder wrapper that stores layerwise activations
class WhisperEncoderOnly(torch.nn.Module):
    def __init__(self, model):
        # Take passed in model (whisper) and grab the encoder portions layers to functionally replicate it
        super().__init__()
        self.conv1 = model.encoder.conv1
        self.conv2 = model.encoder.conv2
        self.positional_embedding = model.encoder.positional_embedding
        self.blocks = model.encoder.blocks

    # Define custom forward pass to extract layer wise activations as sound data is passed through the model
    def forward(self, mel):
        '''
        Parameters: 
            mel 
                - the log mel spectrogram generated by the internal whisper function from desired audio clip
        Returns:
            activations 
                - dictionary containing layer name(s) (key) and said layer's activation data (value)
        '''
        # Create a dictionary that will hold {"layer name" : layer's activations} for each layer
        activations = {}

        # Pass the mel through the first convolutional layer of the encoder
        x = self.conv1(mel)
        # Store this activation
        activations['conv1'] = x.detach().cpu().numpy()

        # Pass into second convolutional layer
        x = self.conv2(x)
        # Store this activation
        activations['conv2'] = x.detach().cpu().numpy()

        # Reshape the conv2 output for the positional embedding layer
        x = x.unsqueeze(0).permute(0, 2, 1)  # (batch, time, dim)

        # Check adherence to expected data shape
        assert x.shape[1:] == self.positional_embedding.shape, "incorrect audio shape"  

        # Pass to the positional_embedding layer
        x = (x + self.positional_embedding).to(x.dtype)
        # Store this activation
        activations['positional_embedding'] = x.detach().cpu().numpy()

        # Pass to the iterative block sequence
        for i, block in tqdm(enumerate(self.blocks)):
            # Pass into block
            x = block(x)
            # To get activations from the blocks 29-31 skip over all other layers
            if i > 28:
                # Store activations of layers targeted for embedding space
                activations[f'layer_{i}'] = np.mean(x.detach().cpu().numpy(), axis=1)

        # Return the dictionary of layer names and their activations
        return activations
    

# (encoder): AudioEncoder(
#     (conv1): Conv1d(128, 1280, kernel_size=(3,), stride=(1,), padding=(1,))
#     (conv2): Conv1d(1280, 1280, kernel_size=(3,), stride=(2,), padding=(1,))
#     (blocks): ModuleList(
#         (0-31): 32 x ResidualAttentionBlock(
#         (attn): MultiHeadAttention(
#             (query): Linear(in_features=1280, out_features=1280, bias=True)
#             (key): Linear(in_features=1280, out_features=1280, bias=False)
#             (value): Linear(in_features=1280, out_features=1280, bias=True)
#             (out): Linear(in_features=1280, out_features=1280, bias=True)
#         )
#         (attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
#         (mlp): Sequential(
#             (0): Linear(in_features=1280, out_features=5120, bias=True)
#             (1): GELU(approximate='none')
#             (2): Linear(in_features=5120, out_features=1280, bias=True)
#         )
#         (mlp_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
#         )
#     )
#     (ln_post): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
# )

# Define file directories
RESULTSDIR = 'C:/Users/ajseidle/Documents/GitHub/auditory_brain_dnn/aud_dnn/full-soundset-actv/Whisper/'
DATADIR = 'C:/Users/ajseidle/Desktop/Spatial Audio/SpatialAudio/SoundDataset_B_NormResamp/'

# Read in the all the wav files in the DATADIR
wav_files = [f for f in os.listdir(DATADIR) if f.lower().endswith('.wav')]

# Load the turbo version of the whisper model
model = whisper.load_model("turbo")
# Assign device to run the model on
device = "cuda" if torch.cuda.is_available() else "cpu"
# Move model to that device
model.to(device)
# Put model in evaluation mode
model.eval()

# Decoder not considered in the original paper's encoder-decoder models    
encoder_only = WhisperEncoderOnly(model).to(device) 

# For each wav file get the activations as it passes through the model
for file in tqdm(wav_files):
    # Define the complete path for this specific wav file
    filepath = os.path.join(DATADIR, file)

    # Load the current wav file using whisper's internal load_audio (handles sampling)
    audio = whisper.load_audio(filepath)

    # Padded to 30 seconds, neccesary for language detection, maybe not critical for our use?
    audio = whisper.pad_or_trim(audio)

    # Make log-Mel spectrogram and move to the same device as the model
    mel = whisper.log_mel_spectrogram(audio, n_mels=model.dims.n_mels).to(model.device)

    # no_grad() to reduce memory usage, not neccesary for evaluation
    with torch.no_grad():
        # Populate the activations dictionary with the output of the custom encoder_only's output 
        activations = encoder_only(mel)

    # Save activations
    output_path = os.path.join(RESULTSDIR, file.replace('.wav', '_activations.pkl'))
    # Write the dictionary to a pickle file
    with open(output_path, 'wb') as f:
        pickle.dump(activations, f)

    # Announce the completion of this file's save
    print(f"Saved encoder activations for {file}")

